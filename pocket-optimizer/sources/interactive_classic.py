"""
æè‡´ç²¾ç®€ç‰ˆÂ·åŸå‘³äº¤äº’æµ‹é‡æº
ä¾èµ–: numpy, scipy (å¯é€‰), sklearn (å¯é€‰, ç”¨äºLOFå¢å¼º)
"""

import numpy as np
import time
from typing import List, Tuple, Optional
from registry import registry

try:
    from scipy import stats
    HAS_SCIPY = True
except ImportError:
    HAS_SCIPY = False

try:
    from sklearn.neighbors import LocalOutlierFactor
    HAS_SKLEARN = True
except ImportError:
    HAS_SKLEARN = False


@registry.register(
    name='source.interactive_classic',
    type_='source',
    signature='measure(point: np.ndarray) -> float'
)
class InteractiveClassicSource:
    """ç²¾ç®€ç‰ˆÂ·åŸå‘³äº¤äº’æµ‹é‡æº - ä¾èµ–åº“å‡½æ•°ï¼Œä»£ç é‡å‡å°‘70%"""
    
    def __init__(self, n_samples: int = 5):
        self.measurement_history = []  # ç”¨äºLOFå¢å¼ºï¼ˆä¸šåŠ¡éœ€è¦ï¼‰
        self.n_samples = n_samples

    def measure(self, point: np.ndarray) -> float:
        # ---------- 1. å‹å¥½çš„å‚æ•°æ˜¾ç¤º ----------
        print(f"\nğŸ“ æµ‹é‡ç‚¹: {self._format_point(point)}")
        print(f"   é‡‡æ ·æ¬¡æ•°: {self.n_samples}")

        # ---------- 2. é€æ¬¡è¾“å…¥æµ‹é‡å€¼ ----------
        raw_values = []
        for i in range(self.n_samples):
            while True:
                try:
                    val = float(input(f"   ç¬¬ {i+1}/{self.n_samples} æ¬¡æµ‹é‡å€¼: "))
                    raw_values.append(val)
                    break
                except ValueError:
                    print("   é”™è¯¯: è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")

        values = np.array(raw_values)

        # ---------- 3. å¤šé‡å¼‚å¸¸å€¼æ£€æµ‹ï¼ˆè‡ªé€‚åº”æ ·æœ¬é‡ï¼‰----------
        filtered = self._filter_outliers(values, point)
        n_orig, n_filt = len(values), len(filtered)
        if n_filt < n_orig:
            print(f"   è¿‡æ»¤æ‰ {n_orig - n_filt} ä¸ªå¼‚å¸¸å€¼")

        # ---------- 4. ç¨³å¥å‡å€¼ä¼°è®¡ï¼ˆä¿®æ•´å‡å€¼ï¼‰----------
        mean_val = self._robust_mean(filtered)
        mean_val = round(mean_val, 6)

        # ---------- 5. ç½®ä¿¡åŒºé—´ï¼ˆtåˆ†å¸ƒ / bootstrapï¼‰----------
        if n_filt >= 2:
            ci = self._confidence_interval(filtered)
            ci_low, ci_high = round(ci[0], 6), round(ci[1], 6)
            width = ci_high - ci_low
            rel_width = width / (abs(mean_val) + 1e-10) * 100
            print(f"   95% ç½®ä¿¡åŒºé—´: [{ci_low:.6f}, {ci_high:.6f}]")
            print(f"   åŒºé—´å®½åº¦: {width:.6f} ({rel_width:.1f}%)")
        else:
            print(f"   æœ‰æ•ˆæ ·æœ¬ä¸è¶³ ({n_filt})ï¼Œç½®ä¿¡åŒºé—´ä¸å¯ç”¨")

        # ---------- 6. è®°å½•å†å²ï¼ˆç”¨äºLOFå¢å¼ºï¼‰----------
        self.measurement_history.append({
            'point': point.copy(),
            'values': raw_values.copy(),
            'mean': mean_val,
            'timestamp': time.time()
        })
        if len(self.measurement_history) > 100:
            self.measurement_history.pop(0)

        print(f"   ç¨³å¥å‡å€¼: {mean_val:.6f}")
        return float(mean_val)

    # ------------------------------------------------------------------
    # ä»¥ä¸‹ä¸ºç²¾ç®€åçš„æ ¸å¿ƒæ–¹æ³•ï¼Œæ¯ä¸ªæ–¹æ³•1-5è¡Œï¼Œå®Œå…¨ä¾èµ–æˆç†Ÿåº“å‡½æ•°
    # ------------------------------------------------------------------

    def _filter_outliers(self, values: np.ndarray, point: np.ndarray) -> np.ndarray:
        """è‡ªé€‚åº”å¼‚å¸¸å€¼æ£€æµ‹ - ç»„åˆIQR, Z-Score, LOFå¢å¼º"""
        n = len(values)
        
        # 1. æå°‘é‡æ ·æœ¬ï¼šLOFå¢å¼ºï¼ˆè‹¥å¯ç”¨ï¼‰
        if n <= 4 and HAS_SKLEARN and len(self.measurement_history) >= 3:
            return self._lof_augmented_filter(values, point)
        
        # 2. é€šç”¨æ£€æµ‹ï¼šIQR + Z-Score
        mask = np.ones(n, dtype=bool)
        
        # IQR æ–¹æ³•ï¼ˆå¯¹éæ­£æ€åˆ†å¸ƒç¨³å¥ï¼‰
        q1, q3 = np.percentile(values, [25, 75])
        iqr = max(q3 - q1, 1e-10)
        lower_iqr = q1 - 1.5 * iqr
        upper_iqr = q3 + 1.5 * iqr
        mask &= (values >= lower_iqr) & (values <= upper_iqr)
        
        # Z-Score æ–¹æ³•ï¼ˆæ­£æ€å‡è®¾ï¼‰
        if HAS_SCIPY and n >= 8:  # æ ·æœ¬é‡è¶³å¤Ÿæ—¶æ‰ç”¨
            z_scores = np.abs(stats.zscore(values, ddof=1))
            mask &= (z_scores < 3)
        else:
            # ç¨³å¥Z-Scoreï¼ˆåŸºäºMADï¼‰
            median = np.median(values)
            mad = np.median(np.abs(values - median))
            if mad > 0:
                robust_z = 0.6745 * (values - median) / mad
                mask &= (np.abs(robust_z) < 3.5)
        
        # 3. ç™¾åˆ†ä½æ•°æˆªæ–­ï¼ˆä¿ç•™95%ï¼‰
        lower_pct = np.percentile(values, 5)
        upper_pct = np.percentile(values, 95)
        mask &= (values >= lower_pct) & (values <= upper_pct)
        
        filtered = values[mask]
        
        # 4. è¿‡åº¦è¿‡æ»¤ä¿æŠ¤ - è‡³å°‘ä¿ç•™2ä¸ªæ ·æœ¬
        if len(filtered) < 2:
            median = np.median(values)
            distances = np.abs(values - median)
            idx = np.argsort(distances)[:2]  # ä¿ç•™ç¦»ä¸­ä½æ•°æœ€è¿‘çš„ä¸¤ä¸ª
            filtered = values[idx]
            print(f"   æ¢å¤ç¦»ä¸­ä½æ•°æœ€è¿‘çš„ {len(filtered)} ä¸ªæ ·æœ¬")
        
        return filtered

    def _lof_augmented_filter(self, values: np.ndarray, point: np.ndarray) -> np.ndarray:
        """LOFå¢å¼ºæ£€æµ‹ - åˆ©ç”¨å†å²æ•°æ®"""
        try:
            # æ„å»ºç‰¹å¾çŸ©é˜µï¼šå½“å‰å€¼ + æœ€è¿‘10ä¸ªå†å²å‡å€¼
            X = np.array([[v, 0] for v in values])
            for hist in self.measurement_history[-10:]:
                X = np.vstack([X, [hist['mean'], 1]])
            
            lof = LocalOutlierFactor(n_neighbors=min(10, len(X)-1), contamination=0.1)
            y_pred = lof.fit_predict(X)
            
            # è¿”å›æœªæ ‡è®°ä¸ºç¦»ç¾¤ç‚¹çš„å½“å‰å€¼
            inliers = [values[i] for i in range(len(values)) if y_pred[i] == 1]
            return np.array(inliers) if inliers else values
        except Exception:
            return values

    def _robust_mean(self, values: np.ndarray) -> float:
        """ç¨³å¥å‡å€¼ - ä½¿ç”¨ä¿®æ•´å‡å€¼ï¼ˆtrimmed meanï¼‰"""
        if HAS_SCIPY:
            # è‡ªé€‚åº”ä¿®æ•´æ¯”ä¾‹ï¼šæ ·æœ¬è¶Šå°‘ï¼Œä¿®æ•´è¶Šå°‘
            trim = min(0.1, 0.5 / len(values)) if len(values) > 2 else 0
            return float(stats.trim_mean(values, trim))
        else:
            # å›é€€ï¼šå»æ‰æœ€å¤§æœ€å°å€¼åå¹³å‡
            if len(values) >= 4:
                return float(np.mean(np.sort(values)[1:-1]))
            return float(np.mean(values))

    def _confidence_interval(self, values: np.ndarray, confidence: float = 0.95) -> Tuple[float, float]:
        """ç½®ä¿¡åŒºé—´ - å°æ ·æœ¬tåˆ†å¸ƒï¼Œå¤§æ ·æœ¬æ­£æ€è¿‘ä¼¼ï¼Œä¸­ç­‰æ ·æœ¬bootstrap"""
        n = len(values)
        if n < 2:
            m = np.mean(values) if n == 1 else 0.0
            return (m, m)
        
        if not HAS_SCIPY:
            # æ— scipyï¼šç™¾åˆ†ä½æ•°æ³•
            return tuple(np.percentile(values, [(1-confidence)/2*100, (1+confidence)/2*100]))
        
        # 1. tåˆ†å¸ƒï¼ˆå°æ ·æœ¬æœ€ä¼˜ï¼‰
        mean = np.mean(values)
        se = np.std(values, ddof=1) / np.sqrt(n)
        ci = stats.t.interval(confidence, df=n-1, loc=mean, scale=se)
        
        # 2. ä¸­ç­‰æ ·æœ¬ç”¨bootstrapéªŒè¯
        if 8 <= n < 50:
            try:
                # scipy 1.8+ æœ‰ bootstrap
                from scipy.stats import bootstrap
                res = bootstrap((values,), np.mean, confidence_level=confidence, 
                                n_resamples=1000, method='BCa')
                boot_ci = res.confidence_interval
                # è‹¥bootstrapåŒºé—´ä¸tåŒºé—´å·®å¼‚>50%ï¼Œé‡‡ç”¨bootstrap
                if abs((boot_ci[1]-boot_ci[0]) - (ci[1]-ci[0])) / (ci[1]-ci[0] + 1e-10) > 0.5:
                    ci = (boot_ci[0], boot_ci[1])
            except (ImportError, AttributeError):
                pass
        
        return ci

    def _format_point(self, point: np.ndarray) -> str:
        """ç®€æ´çš„å‚æ•°æ ¼å¼åŒ–"""
        return ", ".join(f"p{i+1}={v:.6f}" for i, v in enumerate(point))